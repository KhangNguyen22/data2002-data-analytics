---
title: "490503902_Assignment"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(ggplot2)
library(skimr)
library(visdat)
library(gt)
```
# Executive summary
* Item 1
What are we analysing?

* Item 2

## NEED TO FINISH

# Introduction
This report...

## NEED TO FINISH

# Data Cleaning key points


```{r importing, message=FALSE, tidy=TRUE}
# CITATION The following 100 line cleaning code is mostly based on the following work and is only slightly modified for the use of this report.
# Author: Garth Tarr
# Date: 10/09/2020
# Title: Survey data cleaning
# Code Version: 1
# Type: Rmarkdown
# Availability: "https://pages.github.sydney.edu.au/DATA2002/2020/assignment/survey_cleaning.html#"


raw = read_csv("class_survey.csv")
# changes all spaces into underscores and lowercases the whole string for column names
clean_df = raw %>% janitor::clean_names()

## Remove people who automatically submitted without doing survey. Reduce from 174 rows to 172 rows
clean_df <- clean_df[rowSums(is.na(clean_df))<20,]

# Shorten the names of specific columns
colnames(clean_df) = stringr::str_replace(string = colnames(clean_df),
                                   pattern = "what_is_your_",
                                   replacement = ""
                                   )
colnames(clean_df) = stringr::str_replace(string = colnames(clean_df),
                                   pattern = "on_average_how_many_hours_per_week_did_you_",
                                   replacement = ""
                                   )
colnames(clean_df)[2] = "covid_tests"
colnames(clean_df)[4] = "postcode"
colnames(clean_df)[5] = "dentist"
colnames(clean_df)[6] = "university_work"
colnames(clean_df)[7] = "social_media"
colnames(clean_df)[8] = "dog_or_cat"
colnames(clean_df)[9] = "live_with_parents"
colnames(clean_df)[10] = "exercising"
colnames(clean_df)[12] = "asthma"
colnames(clean_df)[13] = "paid_work"
colnames(clean_df)[14] = "fav_season"
colnames(clean_df)[16] = "height"
colnames(clean_df)[17] = "floss_frequency"
colnames(clean_df)[18] = "glasses"
colnames(clean_df)[20] = "steak_preference"
colnames(clean_df)[21] = "stress_level"
## Change postcode to character
clean_df = clean_df %>% mutate(
  postcode = as.character(postcode)
)
## Change all height to cm
clean_df = clean_df %>% dplyr::mutate(
  height = dplyr::case_when(
    height < 2.3 ~ height*100,
    TRUE ~ height
  )
)

## Group gender into 3 categories 

clean_df = clean_df %>% mutate(
  gender = toupper(gender),
  gender = stringr::str_sub(gender, start = 1, end = 1),
  gender = case_when(
    gender == "F" ~ "Female",
    gender == "M" ~ "Male",
    gender == "N" ~ "Non-binary"
  )
)
steak_levels = c("Rare", "Medium-rare", "Medium", 
                 "Medium-well done", "Well done", 
      
                            "I don't eat beef")
clean_df = clean_df %>% 
  mutate(
    steak_preference = factor(steak_preference, levels = steak_levels)
  )
# Change timestamp to POSIXct for easier handling
clean_df$timestamp <-  lubridate::dmy_hms(clean_df$timestamp)

# Create 2 other dataframes based on clean_df ready for hypothesis testing
df_postcode <- clean_df[is.na(clean_df$postcode)==FALSE,]
df_exercising <- clean_df[is.na(clean_df$exercising)==FALSE,]
# glimpse(clean_df)
```


* **Imported class_survey.csv** into R and cleaned the data by shortening the column names and renamed columns. This improves readibility as well as conciseness. 
* **Removed 2 survey observations** where survey was submitted without any input because these observations are useless and do not add any new information to the survey data. 
* **Converted postcode into character** data type as postcode is a categorical variable. I changed all height into cm, grouped gender into 3 categories (male,female and non-binary) and gave an order to steak preferences output.  

* Further, I changed the timestamp variable from character data type to POSIXct data type for easier handling of date time object. 

* After cleaning, there are 3 main data frames used in this report.
  1. **clean_df** = This data frame contains **172 rows** of observations, containing all the observations that have a covid tests value. This data frame will be used in **section 2.4**
  2. **df_postcode** = This data frame is based on the clean_df and contains only **156 rows** of observations due to the removal of NA values in the postcode variable. This dataframe will be used in **section 2.5**.
  3. **df_exercising** =  This data frame is based on the clean_df and contains only **166 rows** of observations due to the removal of NA values in the postcode variable. This dataframe will be used in **section 2.5**.

I only fully cleaned variables(postcode and exercising) for which I will use for our data analysis by removing null values and checking if the values of the variable made sense. 


# Methodology

## Is this a random sample of DATA2002 students?

To answer this question, we need to define what is a random sample. 

A  sample is a subset of population. A random sample is a sample where each person has an equal independent probability of being chosen from a population. 

Yes, this is a sample with only 174 people which is less than the total population, but is it  random?  The test data was collected  through a Google form survey via voluntary choice and was advertised on Edstem. This suggests that the sample is not random as survey participants were more likely to be studious students who read Edstem posts and so is not representative of the true population of DATA2002. Hence, **no**, this is not a random sample of DATA2002. To be random would require a random selection from a database of DATA2002 students that made it compulsory to do the survey.

## What are the potential biases? Which variables are most likely to be subjected to this bias?
**Potential biases**:

* **Selection bias**: Due to the sampling design through voluntary participation, this meant that only motivated students who read the Edstem post were selected to participate. A reduction in selection bias would not occur if a larger sample was taken through greater advertising.


* **Sensitive questions**: Some of the questions in the survey were sensitive questions, which prompted participants to fill in answers that are not objectively true. 
  * The **height variable** would have the tendency for participants to increase their height due to the fear of appearing short. 
  * Another variable would be the **exercising variable** where participants may want to appear fitter by exaggerating how many hours they exercise. 
  * **Paid work** would have a bias of increased hours as participants may want to appear to be hard working.
  * **Glasses** variable may have bias as individuals may be insecure about wearing glasses and so opt not to choose not wearing glasses or contacts.


* **Recall Bias**: **Floss frequency** and **dentist** variable suffer from recall bias as participants personal perception may skew their memory of these variables. The desire to appear hygienic and ideal can make the data inaccurate and unrealistic.



## Are there any questions that needed improvement to generate useful data?

## NEED TO FINISH
Yes!
Shoe size is impossible to clean! Why?


# Results
## Does the number of COVID tests follow a Poisson distribution?

To test this question, we will use a **chi-square goodness-of-fit test** and conduct the test through a hypothesis testing framework at 5% significance level. The **chi-square goodness-of-fit test** was chosen because we have one categorical variable(covid_tests) and one population(DATA2002), which we want to determine whether the data follows a Poisson distribution.

For a Poisson distribution, we need to find the average rate $\lambda$ per unit of time. Since we do not know the exact timeframe in which individuals took covid tests, we can assume that it is finite and is between March 2020 to September 2020. To find lambda sample, we use the following equation: 

$\lambda = \frac{\sum(c*f)}{n}$ where **c** is a categorical variable value, **f** is our corresponding frequency of **c** and **n** is the total number of observations.

```{r, fig.caption="figure 1:covid test frequency table", fig.width=3,fig.height=3,fig.align='center'}
a <- table(clean_df$covid_tests)
col_names <- c(as.numeric(names(a)))
results <- as.vector(a)
freq_df <- data.frame(number_of_covid_test=col_names,freq=results)
# Display the current data frequency
freq_df %>% gt()
```
**Figure 1: covid test frequency table**
```{r, fig.caption="figure 2:covid test frequency bar chart", fig.width=3,fig.height=3,fig.align='center'}
clean_df %>% ggplot(aes(x=covid_tests)) +geom_bar()
```
**Figure 2: covid test frequency bar chart**

Figure 2 bar chart visualises the data in figure 1. We will use the figure 1 to calculate $\lambda$ and Poisson random variable, which will be stored in the p variable.
```{r, fig.cap="This is our lambda value"}
n = sum(results)
num_groups = length(col_names)
lam = sum(results*col_names)/n
print(paste(c("This is our lambda value:",lam), collapse=" ")) 
p <- dpois(col_names,lambda = lam)
print("This is our poisson random variable: p") 
```


**Hypothesis test**

$H_0:$ The number of COVID tests follows a Poisson distribution

$H_1:$ The number of COVID tests does not follow a Poisson distribution

**Assumptions**:

* Observations are independent from each other: YES.
* Expected cell counts for each category of Poisson distribution has a frequency $\geq$ 5. This is a problem which figure 3 shows below.

```{r}
ey = n*p
e_table <- as.table(setNames(round(ey,4),col_names))
knitr::kable(e_table, col.names = c("Number of covid tests  ","Expected Freq"), caption = "Figure 3")
```
Figure 3 shows that covid tests 3,4,5,6 and 10 have a frequency less than 5, so we must combine them into a single group with 2. Hence, we create a new group called "2,3,4,5,6 and 10". Note: our **degrees of freedom** is number of categories -1 - 1(if we needed to estimate lambda) and so is 3-1-1=1.

To calculate p-value,
```{r}
new_results <- c(results[1:2],sum(results[3:8]))
new_ey <- c(ey[1:2],sum(ey[3:8]))
## Adjust p so that probability sums to 1
new_p <- c(p[1:2],1-sum(p[1:2]))
test1 <- chisq.test(new_results,p=new_p)
new_results
## Adjust chi square p value to have a degree of freedom of 1
test1$parameter <- c(df=1)
test1$p.value <- pchisq(test1$statistic,df=test1$parameter,lower.tail=FALSE)
test1
```
**Decision**:
As our p-value is less than 5% significance level, we reject the null hypothesis that the covid test follows the Poisson distribution. This does NOT mean we accept the alternative hypothesis. As we reject the null hypothesis in this report, this does not mean that the true distribution of DATA2002 covid tests does not follow a Poisson distribution. It may be a Poisson distribution if we took a much larger sample from the population.


## Perform two other hypothesis tests. 
### Give some rationale for why you selected these hypothesis tests and interpret the results. Be sure to mention any limitations in the data that may impact your findings.



**Do a chi-square hypothesis test of Independence**
 Lives with Parents v hours a week spent exercising
To test this question, we will use a chi-square...

$H_0:$ Lives
$H_1:$ The number of COVID tests does follow a Poisson distribution

**Assumptions**:

**Need to apply filter**

**Decision**:

**Do a chi-square hypothesis test of Independence**
 Lives with Parents v hours a week spent exercising
To test this question, we will use a chi-square...

$H_0:$ Lives
$H_1:$ The number of COVID tests does follow a Poisson distribution

**Assumptions**:

**Need to apply filter**

**Decision**:

# Conclusion and recommendations